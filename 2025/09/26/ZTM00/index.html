
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>ZTM 00 - PyTorch Fundamentals | My Blog</title>
    <meta name="author" content="Xpyyyy" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>MY BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;MY BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>ZTM 00 - PyTorch Fundamentals</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/26
        </span>
        
        <span class="category">
            <a href="/categories/PyTorch/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                PyTorch
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/PyTorch/" style="color: #00a596">
                    PyTorch
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/DeepLearning/" style="color: #ffa2c4">
                    DeepLearning
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <blockquote>
<p>ZTM : Zero to Mastery Learn PyTorch for Deep Learning</p>
<p>此文档为个人学习笔记，基于<a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">Zero to Mastery Learn PyTorch for Deep Learning</a>.</p>
</blockquote>
<p><strong>What’s ML(DL) ?</strong></p>
<p>机器学习就是把事物（数据）转换成数字然后寻求这些数字中的模式**(finding patterns)**，ML是AI的子集，DL是ML的子集，PyTorch就是聚焦于DeepLearning的编程框架。</p>
<p>传统编程：食材+一条条烹饪法则&#x3D;菜肴</p>
<p>ML算法：食材+菜肴&#x3D;寻求（烹饪法则）</p>
<p><strong>Why use ML or DL?</strong></p>
<p>对于一个复杂的问题，你是否可以想清楚所有的规则呢？大概率不行，所以ML就发挥作用了，<strong>寻求事物数字中的模式</strong></p>
<blockquote>
<p>If you can build a simple rule-based system that doesn’t require machine learning,do that</p>
</blockquote>
<p>并不总是使用ML的，ML不能解决所有问题，如果不需要ML就不必使用</p>
<p><strong>What deep learning is good for?</strong></p>
<ul>
<li>Problems with long lists of rule 传统方法失效，可以尝试MLDL</li>
<li>Continually changing environments DL可以适应新的场景，而不是固定模式</li>
<li>Discovering insights within large collections of data数据量很大的时候，有些任务很难用规则去描述去完成</li>
</ul>
<p><strong>What deep learning is not good for?</strong></p>
<ul>
<li>When you need explainability 机器学习解释性很差，通常使用DL模型学到的模式没有解释性</li>
<li>When the traditional approach is a better option</li>
<li>When errors are unacceptable 机器学习正确率不能达到100%，不能容忍错误的时候不能使用</li>
<li>When you don’t have much data 在数据量小的时候DL模型不能取得很好的效果，欠拟合</li>
</ul>
<p><strong>Machine Learning vs Deep Learning</strong></p>
<p>ML通常更适合结构化的数据，比如表格，每行多个特征+标签XGBoot</p>
<ul>
<li>Random forest</li>
<li>Gradient boosted models</li>
<li>Naive Bayes</li>
<li>Nearest neighbour</li>
<li>Support vector machine</li>
</ul>
<p>DL通常更适合非结构化的数据，比如文本数据，音频数据，图像数据，</p>
<ul>
<li><strong>Neural networks</strong></li>
<li><strong>Fully connected neural network</strong></li>
<li><strong>Convolutional neural network</strong></li>
<li>Recurrent neural network</li>
<li>Transformer</li>
</ul>
<p><strong>What are neural networks?</strong></p>
<p>非结构化数据-&gt;数字表示-&gt;传入神经网络-&gt;数字表示-&gt;解析结果</p>
<p>整体架构：输入层+隐藏层s（学习数据模式）+输出层（预测概率或者学习表示）</p>
<p>每一层都是由线性非线性函数的组合，通过直线和曲线去数据空间刻画模式</p>
<p>Types of Learning</p>
<ul>
<li>监督学习：特征+标签，对于每个输入都有答案</li>
<li>无监督学习&amp;自监督学习：只有特征数据，没有标签</li>
<li>迁移学习，模型从数据集中学到的模式转移到另一个模型</li>
<li>强化学习，会对学习的过程提供奖励和惩罚</li>
<li><strong>这门课专注于监督学习和迁移学习</strong></li>
</ul>
<p><strong>What is deep learning actually used for</strong></p>
<ul>
<li>推荐系统</li>
<li>机器翻译seq2seq(Sequence to Sequence)</li>
<li>语音识别seq2seq</li>
<li>计算机视觉Classification&#x2F;regression</li>
<li>自然语言处理（垃圾邮件检测）Classification&#x2F;regression</li>
</ul>
<p><strong>What is PyTorch</strong></p>
<ul>
<li>Most popular research <strong>deep learning framework</strong></li>
<li>Write fast deep learning code in Python 代码可以运行在GPUs上</li>
<li>Able to access many pre-built deep learning models 使用预训练的模型，迁移学习Torch Hub</li>
<li>Whole stack: preprocess data,model data,deploy model in your application&#x2F;cloud 具有整个完整工作栈</li>
<li>Originally designed and used in-house by Facebook&#x2F;Meta 开源被广泛使用</li>
</ul>
<p>PyTorch可以<strong>通过CUDA接口直接使用GPU</strong></p>
<p>TPU（Tensor Processing Unit）</p>
<p><strong>What is a tensor</strong></p>
<p>Tensor就是DeepLearning中除了网络结构之外的核心</p>
<p>输入数据要转换成tensor塞入神经网络，输出也是tensor的形式，把tensor转换成人类可理解的形式，或许不太严谨地说，目前你可以把tensor当成类似于多维矩阵的东西</p>
<p><strong>What are we going to cover?</strong></p>
<ul>
<li>Now:<ul>
<li>PyTorch basics &amp;fundamentals(dealing with tensors and tensor operations)</li>
</ul>
</li>
<li>Later:<ul>
<li>Preprocessing data(getting it into tensors)</li>
<li>Building and using pretrained deep learning models</li>
<li>Fitting a model to the data(learning patterns)</li>
<li>Making predictions with a model(using patterns)</li>
<li>Evaluating model predictions</li>
<li>Saving and loading models<ul>
<li>Using a trained model to make predictions on custom data</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>ML同时兼具科学和艺术，<strong>艺术可能就是尝试发现效果更好就采用</strong>，<strong>科学有严谨的解释</strong></p>
<p>基本的工作流程：</p>
<ol>
<li><p>Get data ready(turn into tensors)</p>
</li>
<li><p>Build or pick a pretrained model(to suit  your problem)</p>
</li>
</ol>
<p>​	2.1 Pick a loss function &amp;optimizer</p>
<p>​	2.2 Build a training loop</p>
<ol start="3">
<li><p>Fit the model to the data and make a prediction</p>
</li>
<li><p>Evaluate the model</p>
</li>
<li><p>Improve through experimentation</p>
</li>
<li><p>Save and reload your trained model</p>
</li>
</ol>
<p><strong>How to (and how not to) approach this course?</strong></p>
<ol>
<li>Code along</li>
<li>Explore and experiment</li>
<li>Visualize what you don’t understand</li>
<li>Ask questions</li>
<li>Do the exercises</li>
<li>Share your work</li>
</ol>
<p>不要说我学不会什么东西，让自己冷静下来</p>
<p><strong>Important resources for this course</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">Github repo</a></p>
</li>
<li><p>Course Q&amp;A</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">Online book</a></p>
</li>
</ul>
<p><strong>Getting setup to write PyTorch code</strong></p>
<p><a target="_blank" rel="noopener" href="http://www.colab.research.google.com/">www.colab.research.google.com</a></p>
<blockquote>
<p>在谷歌Colab可以直接在线使用GPU，它本质上是一个云端提供服务的Jupyter Notebook软件，有条件可以考虑本地安装jupyter notebook。我这里是使用了<a target="_blank" rel="noopener" href="https://www.kaggle.com/">kaggle notebook</a>，可以一键切换md和code区域，做笔记非常的方便</p>
</blockquote>
<p>Shift + Enter可以执行模块</p>
<p>可以选取GPU作为加速器，使得代码运行在GPU上</p>
<p><code>!nvidia-smi</code>查看GPU信息，<code>!</code>是在jupyter中告诉编译器执行的是shell命令</p>
<p><strong>Introduction to tensors</strong></p>
<pre><code class="language-python">#通常都会import这几个固定库
import torch #深度学习框架，提供强大的张量计算和DNN构建训练功能
import pandas as pd #数据分析处理库，数据组织成数据结构，可以处理csv文件等，可以看作表格
import numpy as np #科学计算基础包，提供多维数组对象和基础数组操作
import matplotlib.pyplot as plt #画图库
print(torch.__version__)
</code></pre>
<p>使用<code>torch.tensor()</code>创建PyTorch里的所有张量，张量是用来表达数据的方式，通常用于高维的数据。</p>
<ul>
<li>标量Scalar<ul>
<li><code>scalar=torch.tensor(7)</code></li>
<li><code>scalar.ndim</code>是张量的维度，标量的维度是0维，矢量是1维，矩阵是2维，可以通过括号的层数来判断维度，矢量<code>[1,2,3,4]</code>，矩阵<code>[[1,2],[2,3]]</code></li>
<li><code>scalar.shape</code>为<code>torch.Size([])</code>，shape就是张量每一维的长度组成的一个列表，因为scalar是0维，所以为空</li>
<li>通过<code>scalar.item()</code>可以把张量转换成int类型，注意只有scalar才可以使用item转换，其他张量会报错</li>
</ul>
</li>
<li>矢量Vector<ul>
<li><code>vector=torch.tensor([7,7])</code></li>
<li>vector的维度是1，有一层括号</li>
<li>vector的shape这里是<code>torch.Size([2])</code></li>
</ul>
</li>
<li>矩阵MATRIX(全大写)<ul>
<li><code>MATRIX=torch.tensor([[7,8],[9,10]])</code></li>
<li>维度是2，两层括号</li>
<li>shape就是<code>torch.Size([2，2])</code></li>
<li>可以通过<code>MATRIX[0]</code>提取出第一维下标为0的张量<code>tensor([7, 8])</code><ul>
<li>以此类推，可以基于第一维的结果再取出第二维的内容</li>
</ul>
</li>
</ul>
</li>
<li>张量TENSOR(大写)<ul>
<li><code>TENSOR=torch.tensor([[[1,2,3],[3,6,9],[2,4,5]]])</code></li>
<li>维度3</li>
<li>shape是<code>torch.Size([1, 3, 3])</code></li>
</ul>
</li>
<li>对于张量，其实可以从最通俗的高维数组来理解，<code>a[x][y][z][q]</code>四维，说明有4层括号，<code>x,y,z,q</code>组成Shape</li>
</ul>
<p><strong>Creating tensors</strong></p>
<blockquote>
<p>约定俗成，矩阵及其更高维的张量名字用大写表示，标量和矢量用小写表示</p>
</blockquote>
<p>Random tensors：为什么需要随机张量，大部分神经网络都会从随机张量开始训练，在训练过程中调整权重使得损失函数的值尽可能小。</p>
<p><code>Start with random numbers-&gt;look at data(Forward)-&gt;update random numbers(Backward) -&gt;look at data -&gt;update random numbers</code></p>
<ul>
<li>随机张量（值的随机化）<ul>
<li><code>random_tensor=torch.rand(3,4)</code>创建3*4的随机张量<ul>
<li>使用<code>rand(1)</code>创建的是<strong>矢量</strong>，注意和标量区分</li>
</ul>
</li>
<li><code>random_image_size_tensor=torch.rand(size=(224,224,3))</code>可以使用参数名<code>size=</code>进行赋值，<code>224*224*3</code>是一个很典型的图片表示的方式，横竖各224像素点，每个点由三个量RGB决定颜色</li>
</ul>
</li>
<li>0张量<ul>
<li><code>zeros=torch.zeros(size=(3,4))</code>，可以不要<code>size=</code></li>
<li>默认类型<code>torch.float32</code></li>
<li><code>*</code>在PyTorch中是逐元素相乘，要求两个张量形状一致</li>
<li>0张量常用于mask操作</li>
</ul>
</li>
<li>1张量<ul>
<li><code>ones=torch.ones(size=(3,4))</code></li>
<li>默认类型<code>torch.float32</code></li>
</ul>
</li>
<li>创建range<ul>
<li><code>one_to_ten=torch.arange(start=1,end=11,step=1)</code>等同<code>arange(0,11)</code>范围<code>[0,11)</code></li>
<li>torch.range在新版本移除，使用arange平替</li>
</ul>
</li>
<li>张量的类比like<ul>
<li><code>ten_zeros=torch.zeros_like(input=one_to_ten)</code><ul>
<li><code>input=</code>可以省略</li>
</ul>
</li>
<li>类似的可以使用<code>ones_like</code></li>
<li>可以复制形状和one_to_ten一模一样，全填0的张量</li>
</ul>
</li>
</ul>
<p><strong>Tensor datatypes</strong></p>
<p>张量运算中有三种类型的错误导致无法执行</p>
<ul>
<li>tensors not right datatype 数据类型不匹配</li>
<li>tensors not right shape 形状不匹配</li>
<li>tensors not on the right device 设备不匹配</li>
</ul>
<pre><code class="language-python">float_32_tensor=torch.tensor([3.0,6.0,9.0],dtype=None,
                             device=&quot;cpu&quot;,#&quot;cuda&quot;即在GPU上
requires_grad=False)#是否记录梯度
#整数默认Int64,浮点默认float32
</code></pre>
<p>实现类型转换</p>
<pre><code class="language-python">float_16_tensor=float_32_tensor.type(torch.float16)
</code></pre>
<p><strong>Tensor attributes</strong></p>
<ul>
<li><code>tensor.dtype</code>获取数据类型</li>
<li><code>tensor.shape</code>获取形状，<code>tensor.size()</code>是一个函数取出shape属性</li>
<li><code>tensor.device</code> 获取所在设备信息，只有张量在同一设备上才可以参与运算</li>
</ul>
<p><strong>Manipulating tensors</strong> (tensor operation)</p>
<ul>
<li><p>基本操作：</p>
<ul>
<li><p>加法</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor+10</code>每一位都加10</li>
<li>得到<code>tensor([11,12,13])</code>不会改变原张量，会生成新的张量</li>
<li><code>tensor=tensor+10</code>才会改变</li>
<li>使用内建函数<code>torch.add(tensor,10)</code>或者<code>tensor.add(10)</code></li>
</ul>
</li>
<li><p>减法</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor-10</code>会得到每一位都减10</li>
<li>得到<code>tensor([-9,-8,-7])</code></li>
</ul>
</li>
<li><p>乘法(逐元素相乘)</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor*10</code>每一位都乘10</li>
<li>得到<code>tensor([10,20,30])</code></li>
<li>使用内建函数<code>torch.mul(tensor,10)</code>或者<code>tensor.mul(10)</code></li>
</ul>
</li>
<li><p>除法</p>
<p>* </p>
</li>
<li><p>矩阵乘法（点积）</p>
<ul>
<li><p><code>torch.matmul(torch1,torch2)</code>，当torch1和torch2都是矢量的时候不用考虑形状，因为pytorch中向量行是固定的，在参与运算的时候会自动视为行或列向量（<code>torch.mm</code>也是一个相同的方法）</p>
</li>
<li><p>在colab中可以使用<code>%%time</code>统计运行时间，在kaggle使用<code>%time</code>统计时间</p>
</li>
<li><p>也可以使用<code>tensor @ tensor</code>，注意矢量使用<code>mm</code>是不可以的，必须作用于矩阵</p>
</li>
<li><p>矩阵相乘的时候<strong>内部维度</strong>要匹配，<code>a*b</code>和<code>b*c</code>相乘才奏效</p>
</li>
<li><p>矩阵相乘的结果矩阵大小为外部维度，<code>a*b</code>和<code>b*c</code>产生<code>a*c</code></p>
</li>
<li><pre><code class="language-python">tensor_A=torch.tensor([[1,2],[3,4],[5,6]])
tensor_B=torch.tensor([[7,10],[8,11],[9,12]])
#转置操作 tensor_B.T
torch.matmul(tensor_A,tensor_B.T)
</code></pre>
</li>
<li><p>矩阵乘法是线代的基础知识，就不多赘述</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>聚合操作：</p>
<ul>
<li><blockquote>
<p>聚合就是用来找出一个张量的最小值，最大值，平均值，等等，从大量数字变成一个数字</p>
</blockquote>
</li>
<li><p>Min</p>
<ul>
<li><code>torch.min(tensor)</code>或者使用<code>tensor.min()</code></li>
</ul>
</li>
<li><p>Max</p>
<ul>
<li><code>torch.max(tensor)</code>或者使用<code>tensor.max()</code></li>
</ul>
</li>
<li><p>Mean</p>
<ul>
<li>无法处理整型张量，需要浮点或者复数，这里就是前面提到的<strong>数据类型不匹配</strong>导致的报错</li>
<li><code>x=torch.arange(0,100,10)</code></li>
<li><code>torch.mean(x.type(torch.float32))</code>或者<code>x.type(torch.float32).mean()</code></li>
</ul>
</li>
<li><p>Sum</p>
<ul>
<li><code>torch.sum(tensor)</code>或者使用<code>tensor.sum()</code></li>
</ul>
</li>
</ul>
</li>
<li><p>找出Min&amp;Max的下标位置</p>
<ul>
<li>Min<ul>
<li><code>x.argmin()</code></li>
</ul>
</li>
<li>Max<ul>
<li><code>x.argmax()</code></li>
</ul>
</li>
</ul>
</li>
<li><p>其他操作</p>
<ul>
<li><p>reshaping</p>
<ul>
<li><p>它返回一个具有新形状的张量，但<strong>不改变原始数据的内容和顺序</strong>，需要保证新旧张量元素个数和一致</p>
</li>
<li><p>相当于按照原来的物理地址排列顺序，把元素一个一个填入新的形状的张量里，view是reshape的子集，reshape有可能共享内存也可能不共享内存（如果张量是非连续的），但是view一定共享内存</p>
</li>
<li><pre><code class="language-python">x=torch.arange(1.,10.)
x,x.shape#(tensor([1.,2.,3.,4.,5.,6.,7.,8.,9.]),torch.Size([9]))

x_reshaped=x.reshape(1,9)
x_reshaped,x_reshaped.shape#(tensor([[1.,2.,3.,4.,5.,6.,7.,8.,9.]]),torch.Size([1,9]))
#多了一个括号，注意这里是矩阵了，跟向量本质不同了

x_reshaped=x.reshape(3,3)
x_reshaped,x_reshaped.shape#(tensor([[1., 2., 3.],[4., 5., 6.],[7., 8., 9.]]),torch.Size([3, 3]))
</code></pre>
</li>
</ul>
</li>
<li><p>view</p>
<ul>
<li>使用一个张量的所在内存的元素构造新的形状为用户可见即视图，但是保持原张量不变，只改变观测角度</li>
<li><code>z=x.view(1,9)</code></li>
<li>使用<code>z[:,0]=5</code> 可以修改z的下标为0的元素为5</li>
<li>都是view是和原始张量<strong>共享内存</strong>的，改变view的元素也会改变原张量的元素</li>
</ul>
</li>
<li><p>stacking</p>
<ul>
<li>这是一个用于<strong>将多个张量（tensors）沿着一个新的维度（dim）堆叠</strong>的函数</li>
<li>所有的张量必须相同形状，它的拼接是创建新维度拼接，而不是类似于(2,3)和(2,3)纵向拼成(4,3) 或 横向拼成(2,6) 这种利用同维度的cat连接，实际上是2个(2,3)拼接成(2,2,3)</li>
<li><code>x_stacked=torch.stack([x,x,x,x],dim=0)</code></li>
<li>dim&#x3D;0是最外层维</li>
<li>dim&#x3D;1是第二维度，依次类推</li>
<li>dim&#x3D;-1是最内层维度</li>
<li>比如x形状<code>torch.size([3])</code>，2个x拼接，dim&#x3D;0的拼接就是(2,3)，dim&#x3D;1的拼接就是(3,2)</li>
<li>可以这样理解k个数组<code>a[i][j]</code>在某个维度上组成一个高一维度的数组，比如dim&#x3D;0，<code>a[k][i][j]</code>对应的数值就是第k个数组的<code>a[i][j]</code>,同样<code>a[i][k][j]</code>对应的也是第k个数组的<code>a[i][j]</code></li>
</ul>
</li>
<li><p>squeeze</p>
<ul>
<li><p><code>y=x.squeeze()</code>or<code>y=torch.squeeze(x)</code></p>
</li>
<li><p>移除张量中长度为1的维度，相当于压缩把<code>(2,1,2,1,2)</code>压缩成<code>(2,2,2)</code>而且不会影响到数据的物理存储</p>
</li>
<li><p>维度为一说明这一维其实是不必要存在的，多了一层括号</p>
</li>
<li><p>也可以选择消除指定的前n层</p>
<p><code>y=torch.squeeze(x,0)</code>不压缩，第二个参数k表示压缩从左到右前k个1</p>
</li>
<li><p>返回的也是<strong>视图</strong>，<strong>共享存储</strong></p>
</li>
</ul>
</li>
<li><p>unsqueeze</p>
<ul>
<li>和squeeze操作相反，增加一维</li>
<li><code>y=x.unsqueeze(dim=0)</code>就在第0维加一维，dim&#x3D;1就在第一维加</li>
</ul>
</li>
<li><p>permute</p>
<ul>
<li>重排维度，比如<code>a[i][j][k]</code>变成<code>a[k][i][j]</code></li>
<li><code>torch.permute(x,(2,0,1))</code>原先的第2维放在现在的第0维，原先的第0维放在现在的第1维，原先的第1维放在现在的第2维</li>
<li>permute之后的对应位置的数据是相同的，比如按照上面的permute方法<code>X_permuted[i][j][k]==X[j][k][i]</code></li>
<li>permute返回的是<strong>视图</strong>，所以改变视图，原数据也会变，<strong>共享存储</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Selecting data(Indexing)</strong></p>
<p>PyTorch的索引和NumPy数据的索引很相似</p>
<pre><code class="language-python">import torch
x=torch.arange(1,10).reshape(1,3,3)
x[0][0][0]#就是tensor(1),等同x[0,0,0]
#可以使用:选取这个维度的所有数据
x[:,:,1]#获取第0第1维所有取值，且第2维下标为1的数据
#注意x[:,1,1]和x[0,1,1]的区别，第一个多一层括号，几个:就有几层括号
#x[0][0]等同x[0,0,:]
</code></pre>
<p><strong>PyTorch and NumPy</strong></p>
<blockquote>
<p>NumPy是一个非常流行的数值计算库，我们会经常在NumPy的数据表示和PyTorch的数据表示之间转换</p>
<p><code>torch.from_numpy(ndarray)</code>可以把<code>array</code>转换成<code>tensor</code></p>
<p><code>torch.Tensor.numpy()</code>可以把<code>tensor</code>转换成<code>numpy</code></p>
</blockquote>
<ul>
<li><p>NumPy array to Tensor</p>
<ul>
<li><pre><code class="language-python">import torch
import numpy
array=np.arange(0.,10.)
tensor=torch.from_numpy(array)
array,tensor
#(array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64))
#注意numpy创建arange的时候默认float64,所以转换成tensor的时候也会使用float64，tensor的arange是默认float32
</code></pre>
</li>
<li><p><code>from_numpy</code>是共享内存的拷贝，如果要新副本需要使用<code>torch.tensor(array)</code>的方式，容易误导的就是如果使用<code>array=array+1</code>的时候tensor不会变，产生了<code>from_numpy</code>不是共享内存的错觉，其实这里array已经产生了一个新的副本加1后再赋值给<code>array</code></p>
</li>
</ul>
</li>
<li><p>Tensor to NumPy</p>
<ul>
<li><pre><code class="language-python">tensor=torch.ones(7)
numpy=tensor.numpy()
tensor,numpy
#(tensor([1., 1., 1., 1., 1., 1., 1.]),array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))
</code></pre>
</li>
<li><p>和上面类似的，这里的拷贝也是<strong>共享内存式</strong>的，可以使用<code>np.array(tensor)</code>创建新副本</p>
</li>
</ul>
</li>
</ul>
<p><strong>Reproducibility</strong></p>
<blockquote>
<p>当我们在创建随机张量的时候，每次运行都是不同的结果，神经网络的训练也是从随机张量开始的，为了方便调试和验证正确性，我们引入了随机种子<strong>random seed</strong>的概念，相当于随机了一个张量后，记录下来，以后都用这个记下的随机数，这就是可重现性</p>
</blockquote>
<pre><code class="language-python">RANDOM_SEED=42
torch.manual_seed(RANDOM_SEED)
ra=torch.rand(3,4)
</code></pre>
<p>此时就可以发现，无论运行多少次，每次生成的ra都是一样的</p>
<p><strong>Accessing a GPU</strong></p>
<blockquote>
<p>GPU意味着更快的数值运算，这一切都归功于CUDA，NVIDIA硬件和PyTorch框架共同协作</p>
</blockquote>
<ul>
<li>Getting s GPU<ul>
<li><strong>最简单的方式</strong>：使用在线免费的GPU，比如Colab,Kaggle</li>
<li>使用自己的GPU，需要设置并且购买GPU</li>
<li>使用云计算，GCP，AWS，Azure等服务器租用云计算机</li>
<li>使用后两种方法需要具体设置PyTorch和CUDA</li>
</ul>
</li>
<li>Check for GPU access with PyTorch<ul>
<li><code>torch.cuda.is_available()</code><ul>
<li>检查是否可以使用PyTorch访问GPU</li>
</ul>
</li>
<li>设置设备无关代码<ul>
<li><code>device=&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</code></li>
<li>如果要使用GPU的时候可以直接使用这个设备变量，也可以把<code>device</code>换成<code>a</code>任何你喜欢的命名</li>
</ul>
</li>
<li>检查设备数<ul>
<li>有时候不止一个GPU</li>
<li><code>torch.cuda.device_count()</code></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 My Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Xpyyyy
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
