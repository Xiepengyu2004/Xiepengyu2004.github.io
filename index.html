
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>My Blog</title>
    <meta name="author" content="Xpyyyy" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>MY BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;MY BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div
        id="home-background"
        ref="homeBackground"
        data-images="/images/background.jpg"
    ></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>My Blog</h1>
                <h3></h3>
                <h5></h5>
            </div>
        </span>
    </div>
</div>
<div
    id="home-posts-wrap"
    ref="homePostsWrap"
    true
>
    <div id="home-posts">
        

<div class="post">
    <a href="/2025/09/29/ZTM01/">
        <h2 class="post-title">ZTM 01 - PyTorch Workflow Fundamentals</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/PyTorch/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                PyTorch
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/29
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <blockquote>
<p>ZTM 00 课后题补充</p>
<p><code>torch.cuda.manual_seed(123)</code>可以为GPU设置随机种子，CPU和每个GPU的随机生成器是独立的，下次在<code>torch.rand(1,3,device=&quot;cuda&quot;)</code>写明后，就会使用GPU自己的随机生成器，但是如果是<code>torch.rand(1,3).to(&quot;cuda&quot;)</code>随机种子就不生效，因为他是在CPU上创建后移动到GPU的</p>
<p><code>a.to(&quot;cuda&quot;)</code>可以把张量移动到GPU上，但是这里不是原地操作，会返回一个新的张量，注意要使用<code>a=</code>进行赋值</p>
</blockquote>
<p><strong>Introduction to PyTorch workflow</strong></p>
<blockquote>
<p>ML包括DL的核心就是：收集过去的数据，建立算法模型去寻找其中的模式，用发现的模式预测未来。</p>
<p>为了简单讲解编程原理和结构组成，这里使用最简单的模型，线性模型</p>
</blockquote>
<ul>
<li>基本的工作流如下<ul>
<li>1 - 准备数据，把现实生活中的数据（语音，图片，文本）转化成tensor形式</li>
<li>2 - 建立或者选取一个预训练模型（模型训练阶段）<ul>
<li>选取损失函数和优化器</li>
<li>建立训练循环</li>
</ul>
</li>
<li>3 - 让模型拟合数据并作预测（模型推理）</li>
<li>4 - 评估模型（模型推理）</li>
<li>5 - 在实验中提升模型性能</li>
<li>6 - 保存或加载自己训练过的模型</li>
</ul>
</li>
</ul>
<p>这一节的主要了解六个步骤的过程，后续再深入讨论</p>
<p><strong>Getting setup for the PyTorch Workflow module</strong></p>
<pre><code class="language-python">what_were_covering=&#123;1 : &quot;data (prepare and load)&quot;,
                    2 : &quot;build model&quot;,
                    3 : &quot;fitting the model to data (training)&quot;,
                    4 : &quot;making predictions and evaluting a model (inference)&quot;,
                    5 : &quot;saving and loading a model&quot;,
                    6 : &quot;putting it all together&quot;
                   &#125;
</code></pre>
<p><code>torch.nn</code>包中有很多神经网络的层（Layer），我们了解完workflow之后的工作就是选取某些层组织起来变成一个神经网络</p>
<p><strong>Creating a dataset with linear regression</strong></p>
<ul>
<li><p>Data</p>
<ul>
<li>数据可以是万物：<ul>
<li>Excel spreadsheet 规范化的行列table</li>
<li>Images of any kind 各种格式的图片</li>
<li>Videos 视频</li>
<li>Audio like songs or podcasts 音频</li>
<li>DNA 基因序列</li>
<li>Text 文本数据</li>
</ul>
</li>
<li>ML的两部曲<ul>
<li>1-数据转换成数字表示（编码）</li>
<li>2-选取或构建模型寻找尽可能好的模式</li>
</ul>
</li>
</ul>
</li>
<li><p>Linear regression</p>
<ul>
<li><p>通常是$Y&#x3D;a+bX$ 这种线性函数，a和b是参数，是模型需要学习的东西</p>
</li>
<li><pre><code class="language-python">import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
from torch import nn
#人为设定的weight,bias,假设这是数据的真实分布，我们用这个分布创建一些数据点
weight=0.7
bias=0.3

start=0
end=1
step=0.02
X=torch.arange(start,end,step).unsqueeze(dim=1)
y=X*weight+bias
X[:10],y[:10],len(X),len(y)

#这就是一个典型的线性模型，X(features)和y(labels)之间是线性关系，但是在现实生活中，我们通常只能收集到很多y、X，然后去寻找它们之间的映射关系，weight和bias是训练出来的（未知）
#取出这里的X和y就可以当成Model的输入，去寻找y=X*weight+bias的模式
</code></pre>
</li>
</ul>
</li>
</ul>
<p><strong>Creating training and test sets</strong></p>
<ul>
<li><p>把数据划分成<strong>训练集，验证集，测试集</strong></p>
<ul>
<li><p>训练集就像平常的课后作业，你有答案有题目，可以通过这种方式学习知识点</p>
</li>
<li><p>验证集就好像自己找了一张模拟卷，模拟考试一下，但是考试时没有答案（考试后可以对答案），通过模拟考试的结果不断调整自己的学习方法等宏观的东西</p>
</li>
<li><p>测试集就好比最终的高考，三年磨一剑，最终只测试一次，不能根据测试的性能去调参</p>
</li>
<li><blockquote>
<p>显然<strong>验证集和测试集</strong>都是模型训练的未见数据，因为训练Model的目的就是使其具有强的泛化能力（就像人可以读没有读过的书，写没有出现过的文章），即对于未见数据理解作答的能力。而验证集和测试集的唯一区别就是，验证集是用来微调模型的模式（<strong>比如超参数，防止过拟合，模型选择等</strong>），以取得更高的正确率或者其他指标，而测试集是现实生活中的任何未见数据是整个模型即将迈入使用的最后一步。</p>
</blockquote>
</li>
<li><p>Training,Validation,Testing通常占比是60-80,10-20,10-20，Validation不是必须的但是通常都有，这里的重点是PyTorch编程，所以使用了简单的数据，也使用了最简单的建模，可以忽略validation，但是在现实实践中，通常这个是不可或缺的。</p>
</li>
<li><pre><code class="language-python">#对于上面的数据我们需要给TrainSet分配40个样本，给TestSet分配10个样本，下面是一个不太严谨但是很简单易懂的划分，可能实际还需要K折验证，随机划分等手段切分数据集
train_split=int(0.8*len(X))
X_train,y_train=X[:train_split],y[:train_split]
X_test,y_test=X[train_split:],y[train_split:]
</code></pre>
</li>
</ul>
</li>
<li><p>可视化函数</p>
<ul>
<li><p>将数据展示出来，使用plt制图</p>
</li>
<li><pre><code class="language-python">def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
    plt.figure(figsize=(10,7))#创建图形窗口
    
    plt.scatter(train_data,train_labels,c=&quot;b&quot;,s=4,label=&quot;Training data&quot;)#scatter是散点图，c是颜色，label是标签，xy轴，plt.plot是折线图
    
    plt.scatter(test_data,test_labels,c=&quot;g&quot;,s=4,label=&quot;Testing data&quot;)
    
    if predictions is not None:
        plt.scatter(test_data,predictions,c=&quot;r&quot;,s=4,label=&quot;Predicting data&quot;)
    
    plt.legend(prop=&#123;&quot;size&quot;:14&#125;)#图例，必须是prop构造参数属性，不能把prop替换成其他
    #plt.show()在Jupyter中会自动呈现就不需要显式调用
    
plot_predictions()
</code></pre>
</li>
<li><p>数据分析师的格言”visualize,visualize,visualize”</p>
</li>
</ul>
</li>
<li><p>建立模型</p>
<ul>
<li><p><code>nn.Module</code> 是一个抽象类，提供了神经网络需要的所有基本功能：</p>
<ul>
<li><strong>参数管理</strong>：自动跟踪所有<strong>可训练参数</strong></li>
<li><strong>层级组织</strong>：可以包含其他模块形成层次结构</li>
<li><strong>设备移动</strong>：方便在 <strong>CPU&#x2F;GPU</strong> 之间移动</li>
<li><strong>序列化</strong>：支持模型的保存和加载</li>
</ul>
</li>
<li><pre><code class="language-python">class LinearRegressionModel(nn.Module):
    
    def __init__(self):
        super().__init__()
        self.weights=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float32))
        self.bias=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float32))#初始化为随机值，因为确实不知道具体的映射关系，梯度grad用于梯度下降调整参数weights和bias
        
    def forward(self,x: torch.Tensor) -&gt; torch.Tensor:#这是类型注解方便理解和编辑器报错，实际写成def forward(self,x):也是一样的结果
        return self.weights*x+self.bias#线性函数，这里不知道原始数据是线性的，刚好尝试用线性回归去拟合一批数据，再去看拟合效果如何
</code></pre>
<ul>
<li>rand和randn的分布不同，rand是0-1的均匀分布，randn是均值为0的标准正态分布</li>
</ul>
</li>
<li><p>梯度下降与反向传播</p>
<ul>
<li>通俗来讲梯度的作用就是基于损失函数，让参数往一个方向变化，使得损失函数下降最快，这个方向由损失函数对参数求导求得即梯度</li>
<li>反向传播就是利用求导的链式法则，把最外层的导数一层一层传递进入内层</li>
<li>这一部分不多赘述可以阅读相关材料理解，PyTorch有内置实现，只需要了解算法原理即可</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Discussing important model building class</strong></p>
<ul>
<li><p><code>torch.nn</code>包含了计算图的所有构建模块，神经网络本身就是一个计算图</p>
</li>
<li><p><code>torch.nn.Prameter</code>模型应该学习的参数，<code>torch.nn</code>的一些layer会为我们设置好</p>
</li>
<li><p><code>torch.nn.Module</code>是所有神经网络模块的基类，子类化它就必须要复写forward</p>
</li>
<li><p><code>torch.optim</code>是优化器，查看梯度后，选择一些算法进行参数的更新操作</p>
</li>
<li><p><code>def forward()</code>所有<code>nn.Module</code>子类需要覆盖的函数，它定义了前向传播的具体形式</p>
</li>
<li><p><code>torch.utils.data.Dataset</code></p>
</li>
<li><p><code>torch.utils.data.DataLoader</code></p>
</li>
</ul>
<p><strong>Checking out the internals of our model</strong></p>
<blockquote>
<p>详细的PyTorch包组件可查阅<a target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/index.html">PyTorch Tutorial</a></p>
</blockquote>
<ul>
<li>Get data ready<ul>
<li><code>torchvision.transforms</code></li>
<li><code>torch.utils.data.Dataset</code></li>
<li><code>torch.utils.data.DataLoader</code></li>
</ul>
</li>
<li>Build or pick a pretrained model<ul>
<li><code>torch.optim</code></li>
<li><code>torch.nn</code></li>
<li><code>torch.nn.Module</code></li>
<li><code>torchvision.models</code></li>
</ul>
</li>
<li>Evaluate the model<ul>
<li><code>torchmetrics</code></li>
</ul>
</li>
<li>Improve through experimentation<ul>
<li><code>torch.utils.tensorboard</code></li>
</ul>
</li>
</ul>
<p>使用<code>.parameters()</code>可以直接获取model的参数，使用<code>state_dict</code>也是类似作用但是提供了名字</p>
<pre><code class="language-Python">torch.manual_seed(42)

model_0=LinearRegressionModel()
list(model_0.parameters())
#[Parameter containing:tensor([0.3367], requires_grad=True),Parameter containing:tensor([0.1288], requires_grad=True)]
model_0.state_dict()#可以获取参数名和值的映射
#OrderedDict([(&#39;weights&#39;, tensor([0.3367])), (&#39;bias&#39;, tensor([0.1288]))])
</code></pre>
<p><strong>Making predictions with our model</strong></p>
<ul>
<li><p><code>torch.inference_mode()</code>可以用来进入推理模式，进行预测</p>
<ul>
<li><p>推理模式关闭了一系列的事物，比如梯度的追踪，或者有些层在推理和训练的过程中的表现是不一样的(Dropout)</p>
</li>
<li><p>当数据输入模型后背后运转的方式是依次调用<code>forward</code>方法</p>
</li>
<li><pre><code class="language-python">with torch.inference_mode():
    y_preds=model_0(X_test)#一次预测
#with语句用于创建一个临时的上下文环境，在这个环境中执行代码块，然后自动清理资源
#torch.inference_mode()函数相当于进入模型推理阶段（返回的是一个上下文管理器对象，所以必须与with联动），后续的模型不会计算梯度
plot_predictions(predictions=y_preds)#绘制带预测的散点图
</code></pre>
</li>
<li><p>还有一种很类似的比较老的方法<code>with torch.no_grad():</code></p>
</li>
</ul>
</li>
</ul>
<p><strong>Training a model with PyTorch (intuition building)</strong></p>
<p>在训练过程中衡量预测值和真实值之间的差距使用损失函数<strong>loss function</strong></p>
<p>分类问题可以选取<code>torch.nn.BCELoss()</code>等，回归问题选取<code>torch.nn.L1Loss()</code>等</p>
<p><strong>optimizer</strong>通过模型损失来调整参数(weights 和bias)</p>
<p>所以我们需要一个Training Loop和Testing Loop</p>
<p><strong>Setting up a loss function and optimizer with PyTorch</strong></p>
<p>PyTorch有内置的很多个损失函数也有很多个优化器，可以具体参考文档</p>
<ul>
<li><p>L1 loss &amp; SGD Optimizer</p>
<ul>
<li><p>$f(x,y)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n|x_i-y_i|$</p>
</li>
<li><p>两种方式求得L1 loss</p>
<ul>
<li><code>MAE_Loss=torch.mean(torch.abs(y_pred-y_test))</code>自己手动实现的版本</li>
<li><code>MAE_Loss=torch.nn.L1Loss</code>使用内置版本</li>
</ul>
</li>
<li><p><code>torch.optim.SGD</code>有两个参数，一个是需要跟踪优化的参数，另一个是学习率（是一种超参数）</p>
</li>
<li><pre><code class="language-python">loss_fn=nn.L1Loss()
optimizer=torch.optim.SGD(params=model_0.parameters(),
                         lr=0.01)
#Loss function&amp;optimizer这两个一定是配合运作的
#学习率是参数变化的幅度，越大参数变化幅度越大，越小变化幅度越小
</code></pre>
</li>
</ul>
</li>
</ul>
<p><strong>PyTorch training loop and testing loop</strong></p>
<p>Training loop的基本步骤</p>
<ol start="0">
<li>Loop through the data</li>
<li>Forward pass (Forward propagation)</li>
<li>Calculate the loss </li>
<li>Optimizer zero grad</li>
<li>Loss backward - move backward through</li>
<li>Optimizer step - use optimizer to adjust the parameters</li>
</ol>
<p>Testing loop的基本步骤</p>
<ol>
<li>Forward pass</li>
<li>Calculate the loss</li>
<li>Calculate evaluation metrics(optional)</li>
</ol>
<p>对于测试阶段，我们不改变参数的值也无需反向传播和优化器步骤，我们只关心它的预测结果以及一些指标</p>
<pre><code class="language-python">epochs=100#100轮训练
train_loss_values = []#用来跟踪训练损失
test_loss_values = []#用来跟踪测试损失
epoch_count = []#轮数

for epoch in range(epochs):
    model_0.train() #设置成训练模式，把所有参数都设置成grad_required
    y_pred=model_0(X_train)#前向传递
    
    loss=loss_fn(y_pred,y_train)#损失计算
    
    optimizer.zero_grad()#梯度置零，因为在backward中每个参数的梯度属性值会累积，在某些常会会用到累积的梯度，但是这里不用
    #所以zero_grad必须在backward之前，优化器只是根据计算图遍历所有的参数更改其内部的属性，它本身不保存梯度
    
    loss.backward()#梯度反向传递
    
    optimizer.step()#优化器优化参数
    
    model_0.eval()#调整成评估模式
    
    with torch.inference_mode():#进入推理模式
        test_pred=model_0(X_test)
        
        test_loss=loss_fn(test_pred,y_test.type(torch.float32))
        
        if epoch%10==0:#十轮记录输出一次模型的状态
            epoch_count.append(epoch)
            train_loss_values.append(loss.detach().numpy)#detach()方法是让loss从计算图中分离出来创建一个新的张量与计算图断开连接，但是数据相同，从而不会影响到原来的计算图
            test_loss_values.append(test_loss.detach().numpy())
            print(f&quot;Epoch: &#123;epoch&#125; | MAE Train Loss: &#123;loss&#125; | MAE Test Loss: &#123;test_loss&#125; &quot;)
        
</code></pre>
<ul>
<li><code>model.train()</code>训练模式，启用Dropout 和BatchNorm的训练期行为</li>
<li><code>model.eval()</code>评估模式，禁用Dropout 和BatchNorm的训练期行</li>
<li><code>torch.inference_mode</code>推理模式，在<code>model.eval()</code>的基础上，禁用梯度的计算</li>
<li>前两个是对象的方法，第三个是框架级别的方法</li>
</ul>
<pre><code class="language-python">plt.plot(epoch_count, train_loss_values, label=&quot;Train loss&quot;)#打印折线图
plt.plot(epoch_count, test_loss_values, label=&quot;Test loss&quot;)
plt.title(&quot;Training and test loss curves&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.xlabel(&quot;Epochs&quot;)
plt.legend();
</code></pre>
<p><strong>Making predictions with a trained PyTorch model (inference)</strong></p>
<p>其实前面已经讲过了，基本类似</p>
<ol>
<li>把模型设置成<code>model.eval</code>其实<code>torch.inference_mode</code>已经可以完成这一步，但是为了代码的可理解性，最好还是这么写</li>
<li><code>with torch.inference_mode()</code>推理模式上下文管理</li>
<li>所有的预测都必须把数据和模型放在同一个设备上，比如同在GPU或CPU</li>
</ol>
<pre><code class="language-python">model_0.eval()

with torch.inference_mode():
  # model_0.to(device)
  # X_test = X_test.to(device)
  y_preds = model_0(X_test)
plot_predictions(predictions=y_preds)
</code></pre>
<p><strong>Saving and loading a PyTorch model</strong></p>
<p>有时候我们需要保存或者加载我们的模型，比如训练完的模型想移动到别的应用上或者给别人使用，训练一半的模型想加载继续训练，有三个方法需要知道</p>
<ul>
<li><code>torch.save</code> 把对象序列化后（使用pickle python的序列化工具）保存到磁盘，模型张量和变量都可以使用这个方法保存</li>
<li><code>torch.load</code>这是一个反序列化操作，把序列化的对象文件加载进内存，可以设置什么设备加载这个对象</li>
<li><code>torch.nn.Module.load_state_dict</code>加载模型的参数字典可以使用<code>state_dict</code>对象，当生产环境和研究中，模型很大通常只保存参数，所以<code>load_state_dict</code>的功能并没有被上面两种方法覆盖，但是需要先创建一个跟原始模型一模一样的model对象再加载参数（最推荐的方式）</li>
<li>使用<code>torch.save(obj=model.state_dict(),f=&quot;...&quot;)</code>保存模型</li>
<li>使用<code>xxxmodel.load_state_dict(torch.load(f))</code>加载模型<ul>
<li><code>torch.load</code>只负责把源文件反序列化成对象<code>OrderedDict</code></li>
<li>接着把对象传递给<code>load_state_dict()</code>方法</li>
</ul>
</li>
</ul>
<pre><code class="language-python">from pathlib import Path

MODEL_PATH = Path(&quot;models&quot;)#只创建Path路径对象，指向&quot;models&quot;目录
MODEL_PATH.mkdir(parents=True, exist_ok=True)#创建models目录，如果父目录不存在也一起创建，如果存在就不报错

# 创建模型存放的路径
MODEL_NAME = &quot;01_pytorch_workflow_model_0.pth&quot;
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 保存模型的字典，save读取两个参数，一个是模型`state_dict()`获取的字典对象，一个是保存的路径
print(f&quot;Saving model to: &#123;MODEL_SAVE_PATH&#125;&quot;)
torch.save(obj=model_0.state_dict(), # 只保存学习到的参数
           f=MODEL_SAVE_PATH) 

#!ls -l models/01_pytorch_workflow_model_0.pth
#可以查看是否保存成功

# 实例化一个新的模型，此时参数的随机初始化的
loaded_model_0 = LinearRegressionModel()

#使用保存的参数进行更新，此时的参数就是之前训练好的参数
loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

# 改变成评估模式
loaded_model_0.eval()

# 模型推理上下文管理器做预测
with torch.inference_mode():
    loaded_model_preds = loaded_model_0(X_test)
    
# 可以比较加载的模型预测结果和原先的模型预测结果，发现一模一样
y_preds == loaded_model_preds
</code></pre>
<p><strong>完整代码如下</strong></p>
<pre><code class="language-python">import torch
import numpy as np
import pandas as pd 
from torch import nn
import matplotlib.pyplot as plt

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

weight = 0.7
bias = 0.3

start=0
end=1
step=0.02
X=torch.arange(start,end,step,dtype=torch.float32).unsqueeze(dim=1)
y=X*weight+bias

train_split=int(0.8*len(X))
X_train=X[:train_split]
y_train=y[:train_split]
X_test=X[train_split:]
y_test=y[train_split:]

def plot_predictions(train_data=X_train,train_labels=y_train,test_data=X_test,
                    test_labels=y_test,predictdata=None):
    plt.figure(figsize=(10,7))
    plt.scatter(train_data,train_labels,c=&quot;b&quot;,s=4,label=&quot;Train_data&quot;)
    plt.scatter(test_data,test_labels,c=&#39;g&#39;,s=4,label=&quot;Test_data&quot;)
    if predictdata is not None:
        plt.scatter(test_data,predictdata,c=&#39;r&#39;,s=4,label=&quot;Predictions&quot;)
    plt.legend(prop=&#123;&quot;size&quot;:14&#125;)
    
class LinearRegressionModelV2(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_layer=nn.Linear(in_features=1,out_features=1)

    def forward(self,x : torch.Tensor) -&gt; torch.Tensor:
        return self.linear_layer(x)

torch.manual_seed(42)
model_1=LinearRegressionModelV2()
model_1.to(device)

loss_fn=nn.L1Loss()
optimizer=torch.optim.SGD(params=model_1.parameters(),lr=0.01)

torch.manual_seed(42)

epochs=1000
X_train=X_train.to(device)
X_test=X_test.to(device)
y_train=y_train.to(device)
y_test=y_test.to(device)

for epoch in range(epochs):
    model_1.train()

    y_pred=model_1(X_train)

    loss=loss_fn(y_pred,y_train)

    optimizer.zero_grad()

    loss.backward()

    optimizer.step()

    model_1.eval()

    with torch.inference_mode():
        y_pre=model_1(X_test)

        test_loss=loss_fn(y_pre,y_test)

    if epoch%100==0:
        print(f&quot;Epoch:&#123;epoch&#125;|Train loss:&#123;loss&#125;|Test loss:&#123;test_loss&#125;&quot;)
        
model_1.eval()

with torch.inference_mode():
    y_pred=model_1(X_test)
plot_predictions(predictdata=y_pred.cpu())

from pathlib import Path

MODULE_PATH=Path(&quot;models&quot;)
MODULE_PATH.mkdir(parents=True,exist_ok=True)

MODULE_NAME=&quot;myfirst.pth&quot;
MODULE_SAVE_PATH=MODULE_PATH / MODULE_NAME

torch.save(obj=model_1.state_dict(),f=MODULE_SAVE_PATH)

model_2=LinearRegressionModelV2()
model_2.load_state_dict(torch.load(MODULE_SAVE_PATH))
model_2.to(device)
model_2.eval()

with torch.inference_mode():
    yload=model_2(X_test)

yload==y_pred
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/PyTorch/" style="color: #03a9f4">PyTorch</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/DeepLearning/" style="color: #00a596">DeepLearning</a>
        </span>
        
    </div>
    <a href="/2025/09/29/ZTM01/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/26/ZTM00/">
        <h2 class="post-title">ZTM 00 - PyTorch Fundamentals</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/PyTorch/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                PyTorch
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/26
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <blockquote>
<p>ZTM : Zero to Mastery Learn PyTorch for Deep Learning</p>
<p>此文档为个人学习笔记，基于<a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">Zero to Mastery Learn PyTorch for Deep Learning</a>.</p>
</blockquote>
<p><strong>What’s ML(DL) ?</strong></p>
<p>机器学习就是把事物（数据）转换成数字然后寻求这些数字中的模式**(finding patterns)**，ML是AI的子集，DL是ML的子集，PyTorch就是聚焦于DeepLearning的编程框架。</p>
<p>传统编程：食材+一条条烹饪法则&#x3D;菜肴</p>
<p>ML算法：食材+菜肴&#x3D;寻求（烹饪法则）</p>
<p><strong>Why use ML or DL?</strong></p>
<p>对于一个复杂的问题，你是否可以想清楚所有的规则呢？大概率不行，所以ML就发挥作用了，<strong>寻求事物数字中的模式</strong></p>
<blockquote>
<p>If you can build a simple rule-based system that doesn’t require machine learning,do that</p>
</blockquote>
<p>并不总是使用ML的，ML不能解决所有问题，如果不需要ML就不必使用</p>
<p><strong>What deep learning is good for?</strong></p>
<ul>
<li>Problems with long lists of rule 传统方法失效，可以尝试MLDL</li>
<li>Continually changing environments DL可以适应新的场景，而不是固定模式</li>
<li>Discovering insights within large collections of data数据量很大的时候，有些任务很难用规则去描述去完成</li>
</ul>
<p><strong>What deep learning is not good for?</strong></p>
<ul>
<li>When you need explainability 机器学习解释性很差，通常使用DL模型学到的模式没有解释性</li>
<li>When the traditional approach is a better option</li>
<li>When errors are unacceptable 机器学习正确率不能达到100%，不能容忍错误的时候不能使用</li>
<li>When you don’t have much data 在数据量小的时候DL模型不能取得很好的效果，欠拟合</li>
</ul>
<p><strong>Machine Learning vs Deep Learning</strong></p>
<p>ML通常更适合结构化的数据，比如表格，每行多个特征+标签XGBoot</p>
<ul>
<li>Random forest</li>
<li>Gradient boosted models</li>
<li>Naive Bayes</li>
<li>Nearest neighbour</li>
<li>Support vector machine</li>
</ul>
<p>DL通常更适合非结构化的数据，比如文本数据，音频数据，图像数据，</p>
<ul>
<li><strong>Neural networks</strong></li>
<li><strong>Fully connected neural network</strong></li>
<li><strong>Convolutional neural network</strong></li>
<li>Recurrent neural network</li>
<li>Transformer</li>
</ul>
<p><strong>What are neural networks?</strong></p>
<p>非结构化数据-&gt;数字表示-&gt;传入神经网络-&gt;数字表示-&gt;解析结果</p>
<p>整体架构：输入层+隐藏层s（学习数据模式）+输出层（预测概率或者学习表示）</p>
<p>每一层都是由线性非线性函数的组合，通过直线和曲线去数据空间刻画模式</p>
<p>Types of Learning</p>
<ul>
<li>监督学习：特征+标签，对于每个输入都有答案</li>
<li>无监督学习&amp;自监督学习：只有特征数据，没有标签</li>
<li>迁移学习，模型从数据集中学到的模式转移到另一个模型</li>
<li>强化学习，会对学习的过程提供奖励和惩罚</li>
<li><strong>这门课专注于监督学习和迁移学习</strong></li>
</ul>
<p><strong>What is deep learning actually used for</strong></p>
<ul>
<li>推荐系统</li>
<li>机器翻译seq2seq(Sequence to Sequence)</li>
<li>语音识别seq2seq</li>
<li>计算机视觉Classification&#x2F;regression</li>
<li>自然语言处理（垃圾邮件检测）Classification&#x2F;regression</li>
</ul>
<p><strong>What is PyTorch</strong></p>
<ul>
<li>Most popular research <strong>deep learning framework</strong></li>
<li>Write fast deep learning code in Python 代码可以运行在GPUs上</li>
<li>Able to access many pre-built deep learning models 使用预训练的模型，迁移学习Torch Hub</li>
<li>Whole stack: preprocess data,model data,deploy model in your application&#x2F;cloud 具有整个完整工作栈</li>
<li>Originally designed and used in-house by Facebook&#x2F;Meta 开源被广泛使用</li>
</ul>
<p>PyTorch可以<strong>通过CUDA接口直接使用GPU</strong></p>
<p>TPU（Tensor Processing Unit）</p>
<p><strong>What is a tensor</strong></p>
<p>Tensor就是DeepLearning中除了网络结构之外的核心</p>
<p>输入数据要转换成tensor塞入神经网络，输出也是tensor的形式，把tensor转换成人类可理解的形式，或许不太严谨地说，目前你可以把tensor当成类似于多维矩阵的东西</p>
<p><strong>What are we going to cover?</strong></p>
<ul>
<li>Now:<ul>
<li>PyTorch basics &amp;fundamentals(dealing with tensors and tensor operations)</li>
</ul>
</li>
<li>Later:<ul>
<li>Preprocessing data(getting it into tensors)</li>
<li>Building and using pretrained deep learning models</li>
<li>Fitting a model to the data(learning patterns)</li>
<li>Making predictions with a model(using patterns)</li>
<li>Evaluating model predictions</li>
<li>Saving and loading models<ul>
<li>Using a trained model to make predictions on custom data</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>ML同时兼具科学和艺术，<strong>艺术可能就是尝试发现效果更好就采用</strong>，<strong>科学有严谨的解释</strong></p>
<p>基本的工作流程：</p>
<ol>
<li><p>Get data ready(turn into tensors)</p>
</li>
<li><p>Build or pick a pretrained model(to suit  your problem)</p>
</li>
</ol>
<p>​	2.1 Pick a loss function &amp;optimizer</p>
<p>​	2.2 Build a training loop</p>
<ol start="3">
<li><p>Fit the model to the data and make a prediction</p>
</li>
<li><p>Evaluate the model</p>
</li>
<li><p>Improve through experimentation</p>
</li>
<li><p>Save and reload your trained model</p>
</li>
</ol>
<p><strong>How to (and how not to) approach this course?</strong></p>
<ol>
<li>Code along</li>
<li>Explore and experiment</li>
<li>Visualize what you don’t understand</li>
<li>Ask questions</li>
<li>Do the exercises</li>
<li>Share your work</li>
</ol>
<p>不要说我学不会什么东西，让自己冷静下来</p>
<p><strong>Important resources for this course</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">Github repo</a></p>
</li>
<li><p>Course Q&amp;A</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">Online book</a></p>
</li>
</ul>
<p><strong>Getting setup to write PyTorch code</strong></p>
<p><a target="_blank" rel="noopener" href="http://www.colab.research.google.com/">www.colab.research.google.com</a></p>
<blockquote>
<p>在谷歌Colab可以直接在线使用GPU，它本质上是一个云端提供服务的Jupyter Notebook软件，有条件可以考虑本地安装jupyter notebook。我这里是使用了<a target="_blank" rel="noopener" href="https://www.kaggle.com/">kaggle notebook</a>，可以一键切换md和code区域，做笔记非常的方便</p>
</blockquote>
<p>Shift + Enter可以执行模块</p>
<p>可以选取GPU作为加速器，使得代码运行在GPU上</p>
<p><code>!nvidia-smi</code>查看GPU信息，<code>!</code>是在jupyter中告诉编译器执行的是shell命令</p>
<p><strong>Introduction to tensors</strong></p>
<pre><code class="language-python">#通常都会import这几个固定库
import torch #深度学习框架，提供强大的张量计算和DNN构建训练功能
import pandas as pd #数据分析处理库，数据组织成数据结构，可以处理csv文件等，可以看作表格
import numpy as np #科学计算基础包，提供多维数组对象和基础数组操作
import matplotlib.pyplot as plt #画图库
print(torch.__version__)
</code></pre>
<p>使用<code>torch.tensor()</code>创建PyTorch里的所有张量，张量是用来表达数据的方式，通常用于高维的数据。</p>
<ul>
<li>标量Scalar<ul>
<li><code>scalar=torch.tensor(7)</code></li>
<li><code>scalar.ndim</code>是张量的维度，标量的维度是0维，矢量是1维，矩阵是2维，可以通过括号的层数来判断维度，矢量<code>[1,2,3,4]</code>，矩阵<code>[[1,2],[2,3]]</code></li>
<li><code>scalar.shape</code>为<code>torch.Size([])</code>，shape就是张量每一维的长度组成的一个列表，因为scalar是0维，所以为空</li>
<li>通过<code>scalar.item()</code>可以把张量转换成int类型，注意只有scalar才可以使用item转换，其他张量会报错</li>
</ul>
</li>
<li>矢量Vector<ul>
<li><code>vector=torch.tensor([7,7])</code></li>
<li>vector的维度是1，有一层括号</li>
<li>vector的shape这里是<code>torch.Size([2])</code></li>
</ul>
</li>
<li>矩阵MATRIX(全大写)<ul>
<li><code>MATRIX=torch.tensor([[7,8],[9,10]])</code></li>
<li>维度是2，两层括号</li>
<li>shape就是<code>torch.Size([2，2])</code></li>
<li>可以通过<code>MATRIX[0]</code>提取出第一维下标为0的张量<code>tensor([7, 8])</code><ul>
<li>以此类推，可以基于第一维的结果再取出第二维的内容</li>
</ul>
</li>
</ul>
</li>
<li>张量TENSOR(大写)<ul>
<li><code>TENSOR=torch.tensor([[[1,2,3],[3,6,9],[2,4,5]]])</code></li>
<li>维度3</li>
<li>shape是<code>torch.Size([1, 3, 3])</code></li>
</ul>
</li>
<li>对于张量，其实可以从最通俗的高维数组来理解，<code>a[x][y][z][q]</code>四维，说明有4层括号，<code>x,y,z,q</code>组成Shape</li>
</ul>
<p><strong>Creating tensors</strong></p>
<blockquote>
<p>约定俗成，矩阵及其更高维的张量名字用大写表示，标量和矢量用小写表示</p>
</blockquote>
<p>Random tensors：为什么需要随机张量，大部分神经网络都会从随机张量开始训练，在训练过程中调整权重使得损失函数的值尽可能小。</p>
<p><code>Start with random numbers-&gt;look at data(Forward)-&gt;update random numbers(Backward) -&gt;look at data -&gt;update random numbers</code></p>
<ul>
<li>随机张量（值的随机化）<ul>
<li><code>random_tensor=torch.rand(3,4)</code>创建3*4的随机张量<ul>
<li>使用<code>rand(1)</code>创建的是<strong>矢量</strong>，注意和标量区分</li>
</ul>
</li>
<li><code>random_image_size_tensor=torch.rand(size=(224,224,3))</code>可以使用参数名<code>size=</code>进行赋值，<code>224*224*3</code>是一个很典型的图片表示的方式，横竖各224像素点，每个点由三个量RGB决定颜色</li>
</ul>
</li>
<li>0张量<ul>
<li><code>zeros=torch.zeros(size=(3,4))</code>，可以不要<code>size=</code></li>
<li>默认类型<code>torch.float32</code></li>
<li><code>*</code>在PyTorch中是逐元素相乘，要求两个张量形状一致</li>
<li>0张量常用于mask操作</li>
</ul>
</li>
<li>1张量<ul>
<li><code>ones=torch.ones(size=(3,4))</code></li>
<li>默认类型<code>torch.float32</code></li>
</ul>
</li>
<li>创建range<ul>
<li><code>one_to_ten=torch.arange(start=1,end=11,step=1)</code>等同<code>arange(0,11)</code>范围<code>[0,11)</code></li>
<li>torch.range在新版本移除，使用arange平替</li>
</ul>
</li>
<li>张量的类比like<ul>
<li><code>ten_zeros=torch.zeros_like(input=one_to_ten)</code><ul>
<li><code>input=</code>可以省略</li>
</ul>
</li>
<li>类似的可以使用<code>ones_like</code></li>
<li>可以复制形状和one_to_ten一模一样，全填0的张量</li>
</ul>
</li>
</ul>
<p><strong>Tensor datatypes</strong></p>
<p>张量运算中有三种类型的错误导致无法执行</p>
<ul>
<li>tensors not right datatype 数据类型不匹配</li>
<li>tensors not right shape 形状不匹配</li>
<li>tensors not on the right device 设备不匹配</li>
</ul>
<pre><code class="language-python">float_32_tensor=torch.tensor([3.0,6.0,9.0],dtype=None,
                             device=&quot;cpu&quot;,#&quot;cuda&quot;即在GPU上
requires_grad=False)#是否记录梯度
#整数默认Int64,浮点默认float32
</code></pre>
<p>实现类型转换</p>
<pre><code class="language-python">float_16_tensor=float_32_tensor.type(torch.float16)
</code></pre>
<p><strong>Tensor attributes</strong></p>
<ul>
<li><code>tensor.dtype</code>获取数据类型</li>
<li><code>tensor.shape</code>获取形状，<code>tensor.size()</code>是一个函数取出shape属性</li>
<li><code>tensor.device</code> 获取所在设备信息，只有张量在同一设备上才可以参与运算</li>
</ul>
<p><strong>Manipulating tensors</strong> (tensor operation)</p>
<ul>
<li><p>基本操作：</p>
<ul>
<li><p>加法</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor+10</code>每一位都加10</li>
<li>得到<code>tensor([11,12,13])</code>不会改变原张量，会生成新的张量</li>
<li><code>tensor=tensor+10</code>才会改变</li>
<li>使用内建函数<code>torch.add(tensor,10)</code>或者<code>tensor.add(10)</code></li>
</ul>
</li>
<li><p>减法</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor-10</code>会得到每一位都减10</li>
<li>得到<code>tensor([-9,-8,-7])</code></li>
</ul>
</li>
<li><p>乘法(逐元素相乘)</p>
<ul>
<li><code>tensor=torch.tensor([1,2,3])</code></li>
<li>使用<code>tensor*10</code>每一位都乘10</li>
<li>得到<code>tensor([10,20,30])</code></li>
<li>使用内建函数<code>torch.mul(tensor,10)</code>或者<code>tensor.mul(10)</code></li>
</ul>
</li>
<li><p>除法</p>
<p>* </p>
</li>
<li><p>矩阵乘法（点积）</p>
<ul>
<li><p><code>torch.matmul(torch1,torch2)</code>，当torch1和torch2都是矢量的时候不用考虑形状，因为pytorch中向量行是固定的，在参与运算的时候会自动视为行或列向量（<code>torch.mm</code>也是一个相同的方法）</p>
</li>
<li><p>在colab中可以使用<code>%%time</code>统计运行时间，在kaggle使用<code>%time</code>统计时间</p>
</li>
<li><p>也可以使用<code>tensor @ tensor</code>，注意矢量使用<code>mm</code>是不可以的，必须作用于矩阵</p>
</li>
<li><p>矩阵相乘的时候<strong>内部维度</strong>要匹配，<code>a*b</code>和<code>b*c</code>相乘才奏效</p>
</li>
<li><p>矩阵相乘的结果矩阵大小为外部维度，<code>a*b</code>和<code>b*c</code>产生<code>a*c</code></p>
</li>
<li><pre><code class="language-python">tensor_A=torch.tensor([[1,2],[3,4],[5,6]])
tensor_B=torch.tensor([[7,10],[8,11],[9,12]])
#转置操作 tensor_B.T
torch.matmul(tensor_A,tensor_B.T)
</code></pre>
</li>
<li><p>矩阵乘法是线代的基础知识，就不多赘述</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>聚合操作：</p>
<ul>
<li><blockquote>
<p>聚合就是用来找出一个张量的最小值，最大值，平均值，等等，从大量数字变成一个数字</p>
</blockquote>
</li>
<li><p>Min</p>
<ul>
<li><code>torch.min(tensor)</code>或者使用<code>tensor.min()</code></li>
</ul>
</li>
<li><p>Max</p>
<ul>
<li><code>torch.max(tensor)</code>或者使用<code>tensor.max()</code></li>
</ul>
</li>
<li><p>Mean</p>
<ul>
<li>无法处理整型张量，需要浮点或者复数，这里就是前面提到的<strong>数据类型不匹配</strong>导致的报错</li>
<li><code>x=torch.arange(0,100,10)</code></li>
<li><code>torch.mean(x.type(torch.float32))</code>或者<code>x.type(torch.float32).mean()</code></li>
</ul>
</li>
<li><p>Sum</p>
<ul>
<li><code>torch.sum(tensor)</code>或者使用<code>tensor.sum()</code></li>
</ul>
</li>
</ul>
</li>
<li><p>找出Min&amp;Max的下标位置</p>
<ul>
<li>Min<ul>
<li><code>x.argmin()</code></li>
</ul>
</li>
<li>Max<ul>
<li><code>x.argmax()</code></li>
</ul>
</li>
</ul>
</li>
<li><p>其他操作</p>
<ul>
<li><p>reshaping</p>
<ul>
<li><p>它返回一个具有新形状的张量，但<strong>不改变原始数据的内容和顺序</strong>，需要保证新旧张量元素个数和一致</p>
</li>
<li><p>相当于按照原来的物理地址排列顺序，把元素一个一个填入新的形状的张量里，view是reshape的子集，reshape有可能共享内存也可能不共享内存（如果张量是非连续的），但是view一定共享内存</p>
</li>
<li><pre><code class="language-python">x=torch.arange(1.,10.)
x,x.shape#(tensor([1.,2.,3.,4.,5.,6.,7.,8.,9.]),torch.Size([9]))

x_reshaped=x.reshape(1,9)
x_reshaped,x_reshaped.shape#(tensor([[1.,2.,3.,4.,5.,6.,7.,8.,9.]]),torch.Size([1,9]))
#多了一个括号，注意这里是矩阵了，跟向量本质不同了

x_reshaped=x.reshape(3,3)
x_reshaped,x_reshaped.shape#(tensor([[1., 2., 3.],[4., 5., 6.],[7., 8., 9.]]),torch.Size([3, 3]))
</code></pre>
</li>
</ul>
</li>
<li><p>view</p>
<ul>
<li>使用一个张量的所在内存的元素构造新的形状为用户可见即视图，但是保持原张量不变，只改变观测角度</li>
<li><code>z=x.view(1,9)</code></li>
<li>使用<code>z[:,0]=5</code> 可以修改z的下标为0的元素为5</li>
<li>都是view是和原始张量<strong>共享内存</strong>的，改变view的元素也会改变原张量的元素</li>
</ul>
</li>
<li><p>stacking</p>
<ul>
<li>这是一个用于<strong>将多个张量（tensors）沿着一个新的维度（dim）堆叠</strong>的函数</li>
<li>所有的张量必须相同形状，它的拼接是创建新维度拼接，而不是类似于(2,3)和(2,3)纵向拼成(4,3) 或 横向拼成(2,6) 这种利用同维度的cat连接，实际上是2个(2,3)拼接成(2,2,3)</li>
<li><code>x_stacked=torch.stack([x,x,x,x],dim=0)</code></li>
<li>dim&#x3D;0是最外层维</li>
<li>dim&#x3D;1是第二维度，依次类推</li>
<li>dim&#x3D;-1是最内层维度</li>
<li>比如x形状<code>torch.size([3])</code>，2个x拼接，dim&#x3D;0的拼接就是(2,3)，dim&#x3D;1的拼接就是(3,2)</li>
<li>可以这样理解k个数组<code>a[i][j]</code>在某个维度上组成一个高一维度的数组，比如dim&#x3D;0，<code>a[k][i][j]</code>对应的数值就是第k个数组的<code>a[i][j]</code>,同样<code>a[i][k][j]</code>对应的也是第k个数组的<code>a[i][j]</code></li>
</ul>
</li>
<li><p>squeeze</p>
<ul>
<li><p><code>y=x.squeeze()</code>or<code>y=torch.squeeze(x)</code></p>
</li>
<li><p>移除张量中长度为1的维度，相当于压缩把<code>(2,1,2,1,2)</code>压缩成<code>(2,2,2)</code>而且不会影响到数据的物理存储</p>
</li>
<li><p>维度为一说明这一维其实是不必要存在的，多了一层括号</p>
</li>
<li><p>也可以选择消除指定的前n层</p>
<p><code>y=torch.squeeze(x,0)</code>不压缩，第二个参数k表示压缩从左到右前k个1</p>
</li>
<li><p>返回的也是<strong>视图</strong>，<strong>共享存储</strong></p>
</li>
</ul>
</li>
<li><p>unsqueeze</p>
<ul>
<li>和squeeze操作相反，增加一维</li>
<li><code>y=x.unsqueeze(dim=0)</code>就在第0维加一维，dim&#x3D;1就在第一维加</li>
</ul>
</li>
<li><p>permute</p>
<ul>
<li>重排维度，比如<code>a[i][j][k]</code>变成<code>a[k][i][j]</code></li>
<li><code>torch.permute(x,(2,0,1))</code>原先的第2维放在现在的第0维，原先的第0维放在现在的第1维，原先的第1维放在现在的第2维</li>
<li>permute之后的对应位置的数据是相同的，比如按照上面的permute方法<code>X_permuted[i][j][k]==X[j][k][i]</code></li>
<li>permute返回的是<strong>视图</strong>，所以改变视图，原数据也会变，<strong>共享存储</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Selecting data(Indexing)</strong></p>
<p>PyTorch的索引和NumPy数据的索引很相似</p>
<pre><code class="language-python">import torch
x=torch.arange(1,10).reshape(1,3,3)
x[0][0][0]#就是tensor(1),等同x[0,0,0]
#可以使用:选取这个维度的所有数据
x[:,:,1]#获取第0第1维所有取值，且第2维下标为1的数据
#注意x[:,1,1]和x[0,1,1]的区别，第一个多一层括号，几个:就有几层括号
#x[0][0]等同x[0,0,:]
</code></pre>
<p><strong>PyTorch and NumPy</strong></p>
<blockquote>
<p>NumPy是一个非常流行的数值计算库，我们会经常在NumPy的数据表示和PyTorch的数据表示之间转换</p>
<p><code>torch.from_numpy(ndarray)</code>可以把<code>array</code>转换成<code>tensor</code></p>
<p><code>torch.Tensor.numpy()</code>可以把<code>tensor</code>转换成<code>numpy</code></p>
</blockquote>
<ul>
<li><p>NumPy array to Tensor</p>
<ul>
<li><pre><code class="language-python">import torch
import numpy
array=np.arange(0.,10.)
tensor=torch.from_numpy(array)
array,tensor
#(array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64))
#注意numpy创建arange的时候默认float64,所以转换成tensor的时候也会使用float64，tensor的arange是默认float32
</code></pre>
</li>
<li><p><code>from_numpy</code>是共享内存的拷贝，如果要新副本需要使用<code>torch.tensor(array)</code>的方式，容易误导的就是如果使用<code>array=array+1</code>的时候tensor不会变，产生了<code>from_numpy</code>不是共享内存的错觉，其实这里array已经产生了一个新的副本加1后再赋值给<code>array</code></p>
</li>
</ul>
</li>
<li><p>Tensor to NumPy</p>
<ul>
<li><pre><code class="language-python">tensor=torch.ones(7)
numpy=tensor.numpy()
tensor,numpy
#(tensor([1., 1., 1., 1., 1., 1., 1.]),array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))
</code></pre>
</li>
<li><p>和上面类似的，这里的拷贝也是<strong>共享内存式</strong>的，可以使用<code>np.array(tensor)</code>创建新副本</p>
</li>
</ul>
</li>
</ul>
<p><strong>Reproducibility</strong></p>
<blockquote>
<p>当我们在创建随机张量的时候，每次运行都是不同的结果，神经网络的训练也是从随机张量开始的，为了方便调试和验证正确性，我们引入了随机种子<strong>random seed</strong>的概念，相当于随机了一个张量后，记录下来，以后都用这个记下的随机数，这就是可重现性</p>
</blockquote>
<pre><code class="language-python">RANDOM_SEED=42
torch.manual_seed(RANDOM_SEED)
ra=torch.rand(3,4)
</code></pre>
<p>此时就可以发现，无论运行多少次，每次生成的ra都是一样的</p>
<p><strong>Accessing a GPU</strong></p>
<blockquote>
<p>GPU意味着更快的数值运算，这一切都归功于CUDA，NVIDIA硬件和PyTorch框架共同协作</p>
</blockquote>
<ul>
<li>Getting s GPU<ul>
<li><strong>最简单的方式</strong>：使用在线免费的GPU，比如Colab,Kaggle</li>
<li>使用自己的GPU，需要设置并且购买GPU</li>
<li>使用云计算，GCP，AWS，Azure等服务器租用云计算机</li>
<li>使用后两种方法需要具体设置PyTorch和CUDA</li>
</ul>
</li>
<li>Check for GPU access with PyTorch<ul>
<li><code>torch.cuda.is_available()</code><ul>
<li>检查是否可以使用PyTorch访问GPU</li>
</ul>
</li>
<li>设置设备无关代码<ul>
<li><code>device=&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</code></li>
<li>如果要使用GPU的时候可以直接使用这个设备变量，也可以把<code>device</code>换成<code>a</code>任何你喜欢的命名</li>
</ul>
</li>
<li>检查设备数<ul>
<li>有时候不止一个GPU</li>
<li><code>torch.cuda.device_count()</code></li>
</ul>
</li>
</ul>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/PyTorch/" style="color: #00bcd4">PyTorch</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/DeepLearning/" style="color: #ffa2c4">DeepLearning</a>
        </span>
        
    </div>
    <a href="/2025/09/26/ZTM00/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/19/hello-world/">
        <h2 class="post-title">Hello World</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Web/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Web
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/19
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="language-bash">$ hexo new &quot;My New Post&quot;
</code></pre>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="language-bash">$ hexo server
</code></pre>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="language-bash">$ hexo generate
</code></pre>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="language-bash">$ hexo deploy
</code></pre>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Hexo/" style="color: #00bcd4">Hexo</a>
        </span>
        
    </div>
    <a href="/2025/09/19/hello-world/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    
    
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar.png" alt="avatar" />
        </div>
        <div class="name">Xpyyyy</div>
        <div class="description">
            <p>Description<br>NJU CS undergraduate student</p>

        </div>
        
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 My Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Xpyyyy
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
</body>
</html>
